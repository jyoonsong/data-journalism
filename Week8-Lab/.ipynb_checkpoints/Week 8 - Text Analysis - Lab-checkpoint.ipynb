{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 - Text Analysis Using NLTK & KoNLPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK를 이용한 영문 분석\n",
    "\n",
    "* NLTK를 사용하기 위해서는 터미널에서 **```pip install nltk```** 를 수행한다.\n",
    "* NLTK는 다수의 샘플데이터와 tagger를 위한 모듈이 포함되어 있다.\n",
    "* 해당 모듈을 다운로드 하기 위해서는 **```nltk.download()```** 를 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# 특정 모듈을 다운로드 하기 위해서는 다음과 같이 해당 모듈의 이름을 패러미터로 제공한다.\n",
    "# nltk.download('gutenberg')  \n",
    "# nltk.download('maxent_treebank_pos_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "gutenberg_files = gutenberg.fileids()\n",
    "gutenberg_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_doc = gutenberg.open('austen-emma.txt').read()\n",
    "gutenberg_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenize the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example from http://www.nltk.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"At eight o'clock on Thursday morning\n",
    "... Arthur didn't feel very good.\"\"\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alphabetical list of part-of-speech tags used in the Penn Treebank Project:\n",
    "https://www.cis.upenn.edu/~treebank/\n",
    "\n",
    "```\n",
    "CC Coordinating conjunction\n",
    "CD Cardinal number\n",
    "DT Determiner\n",
    "EX Existential there\n",
    "FW Foreign word\n",
    "IN Preposition or subordinating conjunction\n",
    "JJ Adjective\n",
    "JJR Adjective, comparative\n",
    "JJS Adjective, superlative\n",
    "LS List item marker\n",
    "MD Modal\n",
    "NN Noun, singular or mass\n",
    "NNS Noun, plural\n",
    "NNP Proper noun, singular\n",
    "NNPS Proper noun, plural\n",
    "PDT Predeterminer\n",
    "POS Possessive ending\n",
    "PRP Personal pronoun\n",
    "PRP$ Possessive pronoun\n",
    "RB Adverb\n",
    "RBR Adverb, comparative\n",
    "RBS Adverb, superlative\n",
    "RP Particle\n",
    "SYM Symbol\n",
    "TO to\n",
    "UH Interjection\n",
    "VB Verb, base form\n",
    "VBD Verb, past tense\n",
    "VBG Verb, gerund or present participle\n",
    "VBN Verb, past participle\n",
    "VBP Verb, non­3rd person singular present\n",
    "VBZ Verb, 3rd person singular present\n",
    "WDT Wh­determiner\n",
    "WP Wh­pronoun\n",
    "WP$ Possessive wh­pronoun\n",
    "WRB Wh­adverb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_tokens = nltk.word_tokenize(gutenberg_doc)\n",
    "gutenberg_tagged = nltk.pos_tag(gutenberg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stemming (or lemmatizing) the Words\n",
    "\n",
    "단어의 어근을 추출하기 위해 stemming 작업을 한다.\n",
    "* NLTK는 stemming과 lemmatizing을 제공하는데, stemming보다는 lemmatazing이 보다 원하는 결과를 얻을 수 있다.\n",
    "https://en.wikipedia.org/wiki/Lemmatisation#Description\n",
    "\n",
    "**Lemmatisation** is closely related to **stemming**. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications.\n",
    "\n",
    "For instance:\n",
    "\n",
    "1. The word \"better\" has \"good\" as its lemma. This link is missed by stemming, as it requires a dictionary look-up.\n",
    "2. The word \"walk\" is the base form for word \"walking\", and hence this is matched in both stemming and lemmatisation.\n",
    "3. The word \"meeting\" can be either the base form of a noun or a form of a verb (\"to meet\") depending on the context, e.g., \"in our last meeting\" or \"We are meeting again tomorrow\". Unlike stemming, lemmatisation can in principle select the appropriate lemma depending on the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "gutenberg_lemma = []\n",
    "for token in gutenberg_tokens:\n",
    "    gutenberg_lemma.append(lemma.lemmatize(token))\n",
    "\n",
    "gutenberg_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_lemma_tagged = nltk.pos_tag(gutenberg_lemma)\n",
    "gutenberg_lemma_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "gutenberg_stemmed = []\n",
    "for token in gutenberg_tokens:\n",
    "    gutenberg_stemmed.append(porter_stemmer.stem(token))\n",
    "\n",
    "gutenberg_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_stemmed_tagged = nltk.pos_tag(gutenberg_stemmed)\n",
    "gutenberg_stemmed_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Stemming and Lemmatization\n",
    "\n",
    "http://stackoverflow.com/questions/17317418/stemmers-vs-lemmatizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer.stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma.lemmatize('running')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 1\n",
    "\n",
    "* 단어별로 카운트를 하여 가장 많이 사용된 순서로 정렬하자.\n",
    "* (참고) https://docs.python.org/3/library/collections.html#collections.Counter.most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "filtered_words = [word for word in gutenberg_lemma if word not in stop_words]\n",
    "filtered_words # i와 I는 다르게 처리된다. 맨 앞에서 .lower() 를 사용하여 모두 소문자로 변환시키면 문제를 해결할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2\n",
    "* 명사의 유니크리스트를 만들어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# KoNLPy 를 이용한 한글 분석\n",
    "\n",
    "* KoNLPy를 이용하여 한글 형태소 분석을 한다.\n",
    "* KoNLPy는 다음과 같은 형태소 분석기를 파이썬에서 사용할 수 있게 한다.\n",
    "    * 한나눔(카이스트, 1999)\n",
    "    * 꼬꼬마(서울대, 2004)\n",
    "    * 코모란(Shineware, 2014)\n",
    "    * 은전한닢 프로젝트 \n",
    "    * Twitter Korean Text\n",
    "* 다음의 주소를 참고하여 KoNLPy를 설치하자.\n",
    "    * http://konlpy.org/en/v0.4.4/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoNLPy의 기초 (from the website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma  # 꼬꼬마 형태소 분석기 사용\n",
    "\n",
    "kkma = Kkma()\n",
    "text = \"오늘 서울의 날씨는 추워질 전망입니다. 오후 한때 소나기가 올 예정입니다.\"\n",
    "sentences = kkma.sentences(text)\n",
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma.nouns(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma.pos(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kolaw\n",
    "fids = kolaw.fileids()\n",
    "fids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_data = kolaw.open('constitution.txt').read()\n",
    "ko_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading Data & Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일 불러오기\n",
    "* data/moon-memorial-day.txt 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/moon-memorial-day.txt', 'r') as f:\n",
    "    lines = f.read().splitlines()\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 빈 문장 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [line for line in lines if line != '']\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 형태소 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형태소 분석 테스트\n",
    "\n",
    "첫번째 문장을 가져와 코모란 형태소 분석기로 분석해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = sentences[0]\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "tagger = Komoran()\n",
    "tags = tagger.pos(sent)\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 문장의 형태소를 분석하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sentences = [tagger.pos(sent) for sent in sentences]\n",
    "tagged_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 명사의 리스트 만들기\n",
    "\n",
    "각 문장의 형태소 중 일반명사(NNG)또는 고유명사(NNP)를 수집하고 카운트를 세어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_list = []\n",
    "for sent in tagged_sentences:    \n",
    "    for word, tag in sent:\n",
    "        if tag in ['NNP', 'NNG']:\n",
    "            noun_list.append(word)\n",
    "            \n",
    "noun_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "noun_counts = Counter(noun_list)\n",
    "noun_counts.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud 그리기\n",
    "* pip install wordcloud\n",
    "* pip install Pillow (PIL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = WordCloud(width=900, height=600, \n",
    "                  font_path='data/08서울남산체 B.ttf',\n",
    "                  background_color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = cloud.fit_words(noun_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 20))\n",
    "plt.axis('off')\n",
    "plt.imshow(cloud)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 같은 문장에 등장하는 단어의 관계도 그리기\n",
    "\n",
    "같은 문장에 등장하는 단어는 서로 연관성이 높을 가능성이 있다.\n",
    "* 예: '돌이켜 보면, 글로벌 경제위기에다 장기 경기 침체로 연속되는 위기에서 벗어나기 위해 매 순간마다 마음을 놓을 수 없었던 순간들이 많았던 것 같습니다.'\n",
    "* '글로벌' '경제위기' '경기' '침체' '위기' -> 서로 연관성이 있는 단어들."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 명사의 unique list 만들기\n",
    "\n",
    "* enumerate() 사용법\n",
    "\n",
    "```\n",
    "choices = ['pizza', 'pasta', 'salad', 'nachos']\n",
    "list(enumerate(choices))\n",
    "=> [(0, 'pizza'), (1, 'pasta'), (2, 'salad'), (3, 'nachos')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_nouns = set()\n",
    "for sent in tagged_sentences:    \n",
    "    for word, tag in sent:\n",
    "        if tag in ['NNP', 'NNG']:\n",
    "            unique_nouns.add(word)\n",
    "\n",
    "unique_nouns = list(unique_nouns)\n",
    "noun_index = {noun: i for i, noun in enumerate(unique_nouns)}\n",
    "noun_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_index['한국전쟁']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장-단어 행렬 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "occurs = np.zeros([len(tagged_sentences), len(unique_nouns)])\n",
    "np.shape(occurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sent in enumerate(tagged_sentences):\n",
    "    for word, tag in sent:\n",
    "        if tag in ['NNP', 'NNG']:\n",
    "            index = noun_index[word]  # 명사가 있으면, 그 명사의 인덱스를 index에 저정\n",
    "            occurs[i][index] = 1  # 문장 i의 index 자리에 1을 채워 넣는다.\n",
    "            \n",
    "occurs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 공존 단어 행렬 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurs = occurs.T.dot(occurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연관 단어의 network graph 그리기\n",
    "\n",
    "* pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "graph = nx.Graph()\n",
    "\n",
    "for i in range(len(unique_nouns)):\n",
    "    for j in range(i + 1, len(unique_nouns)):\n",
    "        if co_occurs[i][j] > 4:\n",
    "            graph.add_edge(unique_nouns[i], unique_nouns[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "layout = nx.spring_layout(graph, k=.5)\n",
    "nx.draw(graph, pos=layout, with_labels=True,\n",
    "        font_size=20, font_family=\"AppleGothic\",\n",
    "        alpha=0.3, node_size=3000)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
