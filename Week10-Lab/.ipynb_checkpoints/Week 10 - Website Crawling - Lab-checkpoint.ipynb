{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 - Website Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifleSoup4 를 이용한 웹페이지 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup 모듈을 로딩한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 텍스트 라인은 샘플 HTML 문서이다. 실제로는 저장된 HTML파일을 불러오거나 웹에서 바로 HTML문서를 다운로드 받아 데이터 처리를 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<html>\\n                <body>\\n                    <h1>Mr. Belvedere Fan Club</h1>\\n                    <div id='nav'>navigation bar</div>\\n                    <div class='nav'>navigation class</div>\\n                    <div class='header'>\\n                        <a href='twitter_anywhere'>my twitter</a>\\n                    </div>\\n                </body>\\n            </html>\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_doc = \"\"\"<html>\n",
    "                <body>\n",
    "                    <h1>Mr. Belvedere Fan Club</h1>\n",
    "                    <div id='nav'>navigation bar</div>\n",
    "                    <div class='nav'>navigation class</div>\n",
    "                    <div class='header'>\n",
    "                        <a href='twitter_anywhere'>my twitter</a>\n",
    "                    </div>\n",
    "                </body>\n",
    "            </html>\"\"\"\n",
    "html_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup의 \"html.parser\"를 이용하여 문서를 parsing한다. \"html.parser\" 외에 \"xml\", \"html5lib\" 등의 parser를 제공한다.\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html>\n",
       "<body>\n",
       "<h1>Mr. Belvedere Fan Club</h1>\n",
       "<div id=\"nav\">navigation bar</div>\n",
       "<div class=\"nav\">navigation class</div>\n",
       "<div class=\"header\">\n",
       "<a href=\"twitter_anywhere\">my twitter</a>\n",
       "</div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(html_doc, \"html.parser\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parsing된 HTML 문서를 보기 좋게 보여준다.\n",
    "\n",
    "range를 사용하여 원하는 만큼만 볼 수 있다. `soup.prettify()[0:100]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 문서에서 ```<h1>``` 인 요소들만 찾아 list로 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading = soup.find_all(\"h1\")\n",
    "heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "요소의 내용(text)를 반환하기 위해서 `get_text()`를 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading[0].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<질문> ```heading.get_text()``` 는 에러가 나는 이유는?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs = soup.find_all(\"div\")\n",
    "divs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러 요소 중, class와 id 등으로 filtering 하기 위해서 두번째 패러미터를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs = soup.find_all(\"div\", class_=\"nav\")\n",
    "divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs = soup.find_all(\"div\", id=\"nav\")\n",
    "divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = []\n",
    "divs = soup.find_all(\"div\", class_=\"header\")\n",
    "for div in divs:\n",
    "    if div.a[\"href\"] == \"twitter_anywhere\":\n",
    "        id_list.append(div.a.text) # text 는 get_text()와 동일하게 사용됨.\n",
    "id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## twitter 아이디와 사용자 이름 수집\n",
    "\n",
    "twtkr_example.html 파일을 읽어 트위터 아이디와 사용자 이름을 수집해 보자. 수집된 id 에서 @ 기호를 삭제하여 출력한다.\n",
    "* 예: u_simin, 유시민"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc = \"\"\n",
    "with open(\"data/twtkr_example.html\") as file:\n",
    "    html_doc = file.read()\n",
    "\n",
    "soup = BeautifulSoup(html_doc, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs = soup.find_all(\"div\", class_=\"header\")\n",
    "for div in divs:\n",
    "    print(div.cite.a[\"href\"].replace(\"/\", \"\"), div.cite.a.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL 가져와서 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "with urllib.request.urlopen(\"https://media.daum.net/ranking/bestreply/\") as url:\n",
    "    doc = url.read()\n",
    "    soup = BeautifulSoup(doc, \"html.parser\")\n",
    "    strongs = soup.find_all(\"strong\", class_=\"tit_thumb\")\n",
    "    for strong in strongs:\n",
    "        print(strong)\n",
    "#         print(strong.a.text)\n",
    "#         print(strong.a[\"href\"])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 1: 데이터 수집을 위한 리스트 작성\n",
    "\n",
    "위의 주소에서 수집하고자 하는 URL의 리스트를 작성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "url_list = []\n",
    "with urllib.request.urlopen(\"https://media.daum.net/ranking/bestreply/\") as url:\n",
    "\n",
    "        \n",
    "url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2: 기사 수집 1\n",
    "\n",
    "#### 주어진 URL의 기사의 타이틀을 수집해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "base_url = \"http://v.media.daum.net/v/20181103172202580\"\n",
    "\n",
    "with urllib.request.urlopen(base_url) as url:\n",
    "    doc = url.read()\n",
    "    soup = BeautifulSoup(doc, \"html.parser\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주어진 기사의 본문을 수집해 보자.\n",
    "\n",
    "이 기사에는 figurecaption 부분이 있는데, 다음과 같은 코드로 제외한다. \n",
    "\n",
    "`unwanted = div.find(\"p\", class_=\"link_figure\")\n",
    "unwanted.extract()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "base_url = \"http://v.media.daum.net/v/20181103172202580\"\n",
    "\n",
    "with urllib.request.urlopen(base_url) as url:\n",
    "    doc = url.read()\n",
    "    soup = BeautifulSoup(doc, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기자이름과 입력 날짜를 수집해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from datetime import datetime as dt\n",
    "\n",
    "base_url = \"http://v.media.daum.net/v/20181103172202580\"\n",
    "\n",
    "with urllib.request.urlopen(base_url) as url:\n",
    "    doc = url.read()\n",
    "    soup = BeautifulSoup(doc, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 언론사 이름을 수집해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "base_url = \"http://v.media.daum.net/v/20181103172202580\"\n",
    "\n",
    "with urllib.request.urlopen(base_url) as url:\n",
    "    doc = url.read()\n",
    "    soup = BeautifulSoup(doc, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 3: 기사 수집 2\n",
    "\n",
    "#### 위의 내용을 바탕으로 댓글 많은 뉴스 전체 기사를 수집해서 JSON으로 저장해 보자.\n",
    "1. 위의 코드를 합쳐서\n",
    "    - 기사의 리스트 수집\n",
    "    - 각 기사 내용 수집\n",
    "2. JSON 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON 파일 만들기\n",
    "\n",
    "```\n",
    "import json\n",
    "from collections import OrderedDict\n",
    " \n",
    "# Ready for data\n",
    "group_data = OrderedDict()\n",
    "items = OrderedDict()\n",
    " \n",
    "group_data[\"url\"] = \"http://......\"\n",
    "group_data[\"title\"] = \"article_title\"\n",
    " \n",
    "items[\"item1\"] = \"item_name1\"\n",
    "items[\"item2\"] = \"item_name2\"\n",
    "items[\"item3\"] = \"item_name3\"\n",
    " \n",
    "group_data[\"items\"] = items\n",
    " \n",
    "# Print JSON\n",
    "print(json.dumps(group_data, ensure_ascii=False, indent=\"\\t\") )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
