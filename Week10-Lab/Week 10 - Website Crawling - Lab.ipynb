{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 - Website Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifleSoup4 를 이용한 웹페이지 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup 모듈을 로딩한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 텍스트 라인은 샘플 HTML 문서이다. 실제로는 저장된 HTML파일을 불러오거나 웹에서 바로 HTML문서를 다운로드 받아 데이터 처리를 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<html>\\n                <body>\\n                    <h1>Mr. Belvedere Fan Club</h1>\\n                    <div id='nav'>navigation bar</div>\\n                    <div class='nav'>navigation class</div>\\n                    <div class='header'>\\n                        <a href='twitter_anywhere'>my twitter</a>\\n                    </div>\\n                </body>\\n            </html>\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_doc = \"\"\"<html>\n",
    "                <body>\n",
    "                    <h1>Mr. Belvedere Fan Club</h1>\n",
    "                    <div id='nav'>navigation bar</div>\n",
    "                    <div class='nav'>navigation class</div>\n",
    "                    <div class='header'>\n",
    "                        <a href='twitter_anywhere'>my twitter</a>\n",
    "                    </div>\n",
    "                </body>\n",
    "            </html>\"\"\"\n",
    "html_doc # 따옴표가 있는 string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup의 \"html.parser\"를 이용하여 문서를 parsing한다. \"html.parser\" 외에 \"xml\", \"html5lib\" 등의 parser를 제공한다.\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html>\n",
       "<body>\n",
       "<h1>Mr. Belvedere Fan Club</h1>\n",
       "<div id=\"nav\">navigation bar</div>\n",
       "<div class=\"nav\">navigation class</div>\n",
       "<div class=\"header\">\n",
       "<a href=\"twitter_anywhere\">my twitter</a>\n",
       "</div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(html_doc, \"html.parser\")\n",
    "soup # 겉보기엔 차이가 없으나 DOM object화된 상태. 따옴표 없음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "print(type(html_doc)) # string \n",
    "print(type(soup)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parsing된 HTML 문서를 보기 좋게 보여준다.\n",
    "\n",
    "range를 사용하여 원하는 만큼만 볼 수 있다. `soup.prettify()[0:100]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <body>\n",
      "  <h1>\n",
      "   Mr. Belvedere Fan Club\n",
      "  </h1>\n",
      "  <div id=\"nav\">\n",
      "   navigation bar\n",
      "  </div>\n",
      "  <div class=\"nav\">\n",
      "   navigation class\n",
      "  </div>\n",
      "  <div class=\"header\">\n",
      "   <a href=\"twitter_anywhere\">\n",
      "    my twitter\n",
      "   </a>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 문서에서 ```<h1>``` 인 요소들만 찾아 list로 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h1>Mr. Belvedere Fan Club</h1>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading = soup.find_all(\"h1\")\n",
    "heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "요소의 내용(text)를 반환하기 위해서 `get_text()`를 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mr. Belvedere Fan Club'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading[0].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<질문> ```heading.get_text()``` 는 에러가 나는 이유는?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div id=\"nav\">navigation bar</div>,\n",
       " <div class=\"nav\">navigation class</div>,\n",
       " <div class=\"header\">\n",
       " <a href=\"twitter_anywhere\">my twitter</a>\n",
       " </div>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divs = soup.find_all(\"div\")\n",
    "divs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러 요소 중, class와 id 등으로 filtering 하기 위해서 두번째 패러미터를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"nav\">navigation class</div>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divs = soup.find_all(\"div\", class_=\"nav\")\n",
    "divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div id=\"nav\">navigation bar</div>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divs = soup.find_all(\"div\", id=\"nav\")\n",
    "divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my twitter']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_list = []\n",
    "divs = soup.find_all(\"div\", class_=\"header\")\n",
    "for div in divs:\n",
    "    if div.a[\"href\"] == \"twitter_anywhere\":\n",
    "        id_list.append(div.a.text) # text 는 get_text()와 동일하게 사용됨.\n",
    "id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## twitter 아이디와 사용자 이름 수집\n",
    "\n",
    "twtkr_example.html 파일을 읽어 트위터 아이디와 사용자 이름을 수집해 보자. 수집된 id 에서 @ 기호를 삭제하여 출력한다.\n",
    "* 예: u_simin, 유시민"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc = \"\"\n",
    "with open(\"data/twtkr_example.html\", encoding = \"utf-8\") as file: # 윈도우에서는 encoding = \"utf-8\" 옵션 주기\n",
    "    html_doc = file.read()\n",
    "\n",
    "soup = BeautifulSoup(html_doc, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_simin 유시민\n",
      "BBK_Sniper 정봉주\n",
      "funronga 김용민\n",
      "actormoon 문성근 (민주당,배우)\n",
      "heenews 이정희 (통합진보당)\n",
      "GH_PARK 박근혜 (새누리당)\n",
      "HanMyeongSook 한명숙 (민주통합당)\n",
      "moonriver365 문재인\n",
      "sangjungsim 심상정\n",
      "your_rights 최재천 (민주통합당)\n",
      "kangkumsil 강금실\n",
      "mentshin 신경민 (민주당,前앵커)\n",
      "kanggigap 강기갑\n",
      "Nakw 나경원 (새누리당)\n",
      "Jungwook_Hong 홍정욱 (새누리당)\n",
      "cheolsoo0919 안철수\n",
      "jwp615 박지원 (민주통합당)\n",
      "hcroh 노회찬\n",
      "gihos1 서기호\n",
      "hongshenx 홍세화 (진보신당)\n"
     ]
    }
   ],
   "source": [
    "divs = soup.find_all(\"div\", class_=\"header\")\n",
    "for div in divs:\n",
    "    print(div.cite.a[\"href\"].replace(\"/\", \"\"), div.cite.a.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL 가져와서 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "박찬주 \"원하면 '충남 천안을'..군인권센터 소장 삼청교육대 가야\"(종합)\n",
      "박찬주 前대장 \"저를 필요로 하지 않으면 나설 이유 없다\"\n",
      "거세진 세월호 참사 전면 재수사 목소리..\"참사 책임자 122명 고발\"\n",
      "전 직원 매일 야근·출장?..주민센터의 수당 빼돌리기\n",
      "간호조무사 1만명, 국회 앞 모여 \"직종 차별 중단하라\"\n",
      "[현장영상] 박찬주 \"공관병 갑질 사건은 왜곡\" 억울함 호소\n",
      "'한국당 영입 논란' 박찬주, 오늘 기자회견..\"나라 바로 세울 것\"\n",
      "리얼미터 \"문 대통령 국정지지도, 긍정 47.5%·부정 49.1%..3주 연속 오름세\n",
      "한국당 \"강기정 폭력·행패 하루 이틀 아냐..제 버릇 개 못 줘\"\n",
      "'한국당 영입 무산' 박찬주 \"나라 바로 세우겠다\"\n",
      "군인권센터 \"박근혜 청와대, '계엄령 문건' 관여 추가정황 확인\"\n",
      "黃 \"박찬주 재영입 하겠다\".. 당내 반발에도 마이웨이\n",
      "[단독]노후 경유차 취득세, 2배로 늘어난다\n",
      "檢 '조국 민정수석실, 유재수 감찰무마' 금융위 압수수색\n",
      "文대통령-아베, 11분 단독환담..\"실질 관계진전 방안 도출 희망\"\n",
      "유승민, 文대통령에 \"강기정 해임하고 국회에 사과해야\"\n",
      "이철희 \"15~20명 불출마 염두..쇄신 없으면 사퇴\"\n",
      "박찬주 \"군인권센터 해체해야..임태훈 삼청교육대 교육 필요\"\n",
      "백경훈, '한국당 영입 세습' 논란에 \"부끄러울 이유 없다\"\n",
      "文대통령 채근에.. 검찰개혁 속도전 잇단 무리수\n",
      "'이해찬 체제론 총선 어렵다'.. 여당, 커지는 '이낙연 등판론'\n",
      "[단독]\"손가락 때문에 7명 숨져\"..추락헬기에도 저주의 댓글\n",
      "옆자리로 아베 손 이끈 文대통령..11분 '즉석환담' 냉기류 반전\n",
      "\"깜깜이 학종, 이제그만\" vs \"정시로는 아인슈타인 안나와\"\n",
      "청주서 주차된 일본차 화학 물질 손상..경찰 수사\n",
      "\"구조 위해 달라했는데..\" KBS 추락 전 영상 논란\n",
      "울면서 후회해도 소용없다. 손흥민의 잘못은 팩트다\n",
      "'82년생 김지영'에 2030男, 왜 분노하는가\n",
      "靑국감 파행 부른 '버럭 참모들'.. 與서도 \"이러니 교체설 나와\"\n",
      "日언론, 文-아베 조우 주목..\"웃는 얼굴 수초 접촉\"\n",
      "황교안, 박찬주 영입강행 시사..'리더십 논란' 정면돌파하나\n",
      "'진보 유튜버'황희두·양정철·금태섭 등 與 총선기획단 위원\n",
      "\"재판 빨리 받고 싶다\"..'마약 소지·투약 혐의' 홍정욱 前의원 딸\n",
      "안철수, 베를린 이어 '뉴욕마라톤대회' 완주..3시간59분(종합)\n",
      "김종대 \"이자스민 2012년 민주당 입당신청..안받아줘 새누리行\"\n",
      "홍준표 \"들쥐 정치\" 직격탄..황교안 잇단 리더십 논란\n",
      "대화하는 문 대통령과 아베 총리\n",
      "홍준표 \"박찬주는 5공 시대나 어울려..지금 시대 부적절\"\n",
      "\"기무사령관, 박근혜 탄핵날 독대..靑에 군조치 보고도\"\n",
      "대안신당, 신당 명칭·색상 결정..\"김대중 상징 진녹색\"(종합)\n",
      "황교안 \"靑 오만함 극에 달해..내각과 함께 전면 개편해야\"\n",
      "[악플공화국] 잡고보니 엄마·아빠·부장님..4060 '시니어 악플러'의 민낯\n",
      "한국당 떠나 정의당 가는 이자스민..심상정이 직접 챙겼다\n",
      "나경원, 오늘 '패스트트랙 충돌' 의견서 檢 제출..\"국감 뒤 출석\"\n",
      "[문재인 정부 임기 반환점] 극단적 팬덤정치로 치명상.. 등돌린 산토끼에 달렸다\n",
      "전북경찰청 \"순경의 성관계 동영상 실체 확인\"..수사로 전환\n",
      "실종자 가족들 \"'펑' 소리 후 추락 영상 봤다\"..해경은 부인(종합)\n",
      "검찰, '유재수 감찰무마' 수사 속도..금융위 등 압수수색(종합)\n",
      "자궁경부암 발생 연령이 점점 낮아지는 이유는\n",
      "<여의도 왁자지껄>이자스민 정의당행에..장제원·금태섭 \"자성해야\"\n",
      "'ㅋㅋ' 치면 곰돌이가 빵긋..10대 소녀 취향저격한 모바일키보드 \n",
      "英찰스 왕세자 저택 753억 모네 그림, 알고보니 짝퉁?\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "with urllib.request.urlopen(\"https://media.daum.net/ranking/bestreply/\") as url:\n",
    "    doc = url.read()\n",
    "    soup = BeautifulSoup(doc, \"html.parser\")\n",
    "    strongs = soup.find_all(\"strong\", class_=\"tit_thumb\")\n",
    "    for strong in strongs:\n",
    "#         print(strong)\n",
    "        print(strong.a.text)\n",
    "#         print(strong.a[\"href\"])\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 1: 데이터 수집을 위한 리스트 작성\n",
    "\n",
    "위의 주소에서 수집하고자 하는 URL의 리스트를 작성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://v.media.daum.net/v/20191104124132330',\n",
       " 'http://v.media.daum.net/v/20191103160634408',\n",
       " 'http://v.media.daum.net/v/20191103213603813',\n",
       " 'http://v.media.daum.net/v/20191103203615362',\n",
       " 'http://v.media.daum.net/v/20191103152229524',\n",
       " 'http://v.media.daum.net/v/20191104103618431',\n",
       " 'http://v.media.daum.net/v/20191104053006720',\n",
       " 'http://v.media.daum.net/v/20191104081014525',\n",
       " 'http://v.media.daum.net/v/20191103173435117',\n",
       " 'http://v.media.daum.net/v/20191103185243296',\n",
       " 'http://v.media.daum.net/v/20191104102409857',\n",
       " 'http://v.media.daum.net/v/20191104031128946',\n",
       " 'http://v.media.daum.net/v/20191104060013922',\n",
       " 'http://v.media.daum.net/v/20191104114527165',\n",
       " 'http://v.media.daum.net/v/20191104122011846',\n",
       " 'http://v.media.daum.net/v/20191104091802220',\n",
       " 'http://v.media.daum.net/v/20191103211813669',\n",
       " 'http://v.media.daum.net/v/20191104121419650',\n",
       " 'http://v.media.daum.net/v/20191104120201106',\n",
       " 'http://v.media.daum.net/v/20191104044342420',\n",
       " 'http://v.media.daum.net/v/20191104040707291',\n",
       " 'http://v.media.daum.net/v/20191104084754293',\n",
       " 'http://v.media.daum.net/v/20191104135439085',\n",
       " 'http://v.media.daum.net/v/20191104094804311',\n",
       " 'http://v.media.daum.net/v/20191104122348945',\n",
       " 'http://v.media.daum.net/v/20191103202016223',\n",
       " 'http://v.media.daum.net/v/20191104104301703',\n",
       " 'http://v.media.daum.net/v/20191104100336933',\n",
       " 'http://v.media.daum.net/v/20191104030134673',\n",
       " 'http://v.media.daum.net/v/20191104085637496',\n",
       " 'http://v.media.daum.net/v/20191104115508688',\n",
       " 'http://v.media.daum.net/v/20191104120255169',\n",
       " 'http://v.media.daum.net/v/20191104130945865',\n",
       " 'http://v.media.daum.net/v/20191104084538252',\n",
       " 'http://v.media.daum.net/v/20191104100849196',\n",
       " 'http://v.media.daum.net/v/20191103210307550',\n",
       " 'http://v.media.daum.net/v/20191104122151917',\n",
       " 'http://v.media.daum.net/v/20191104131315961',\n",
       " 'http://v.media.daum.net/v/20191104112331711',\n",
       " 'http://v.media.daum.net/v/20191104114606208',\n",
       " 'http://v.media.daum.net/v/20191104095003385',\n",
       " 'http://v.media.daum.net/v/20191104100110818',\n",
       " 'http://v.media.daum.net/v/20191103161729583',\n",
       " 'http://v.media.daum.net/v/20191104125153503',\n",
       " 'http://v.media.daum.net/v/20191104041157325',\n",
       " 'http://v.media.daum.net/v/20191104111330046',\n",
       " 'http://v.media.daum.net/v/20191104125056484',\n",
       " 'http://v.media.daum.net/v/20191104120731438',\n",
       " 'http://v.media.daum.net/v/20191104060227117',\n",
       " 'http://v.media.daum.net/v/20191103155953281',\n",
       " 'http://v.media.daum.net/v/20191104151214858',\n",
       " 'http://v.media.daum.net/v/20191104150356412']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url_list = []\n",
    "with urllib.request.urlopen(\"https://media.daum.net/ranking/bestreply/\") as url:\n",
    "    doc = url.read()\n",
    "    soup = BeautifulSoup(doc, \"html.parser\")\n",
    "    strongs = soup.find_all(\"strong\", class_=\"tit_thumb\")\n",
    "    for strong in strongs:\n",
    "        url_list.append(strong.a[\"href\"])\n",
    "\n",
    "url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2: 기사 수집 1\n",
    "\n",
    "#### 주어진 URL의 기사의 타이틀을 수집해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'나경원 \"역사 부인하는 아베 발언 치졸..사과부터 하라\"'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "base_url = \"http://v.media.daum.net/v/20181103172202580\"\n",
    "\n",
    "with urllib.request.urlopen(base_url) as url:\n",
    "    doc = url.read()\n",
    "    soup = BeautifulSoup(doc, \"html.parser\")\n",
    "    title = soup.find(\"h3\", class_=\"tit_view\").text\n",
    "\n",
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주어진 기사의 본문을 수집해 보자.\n",
    "\n",
    "이 기사에는 figurecaption 부분이 있는데, 다음과 같은 코드로 제외한다. \n",
    "\n",
    "`unwanted = div.find(\"p\", class_=\"link_figure\")\n",
    "unwanted.extract()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "base_url = \"http://v.media.daum.net/v/20181103172202580\"\n",
    "\n",
    "with urllib.request.urlopen(base_url) as url:\n",
    "    doc = url.read()\n",
    "    soup = BeautifulSoup(doc, \"html.parser\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기자이름과 입력 날짜를 수집해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from datetime import datetime as dt\n",
    "\n",
    "base_url = \"http://v.media.daum.net/v/20181103172202580\"\n",
    "\n",
    "with urllib.request.urlopen(base_url) as url:\n",
    "    doc = url.read()\n",
    "    soup = BeautifulSoup(doc, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 언론사 이름을 수집해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "base_url = \"http://v.media.daum.net/v/20181103172202580\"\n",
    "\n",
    "with urllib.request.urlopen(base_url) as url:\n",
    "    doc = url.read()\n",
    "    soup = BeautifulSoup(doc, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 3: 기사 수집 2\n",
    "\n",
    "#### 위의 내용을 바탕으로 댓글 많은 뉴스 전체 기사를 수집해서 JSON으로 저장해 보자.\n",
    "1. 위의 코드를 합쳐서\n",
    "    - 기사의 리스트 수집\n",
    "    - 각 기사 내용 수집\n",
    "2. JSON 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON 파일 만들기\n",
    "\n",
    "```\n",
    "import json\n",
    "from collections import OrderedDict\n",
    " \n",
    "# Ready for data\n",
    "group_data = OrderedDict()\n",
    "items = OrderedDict()\n",
    " \n",
    "group_data[\"url\"] = \"http://......\"\n",
    "group_data[\"title\"] = \"article_title\"\n",
    " \n",
    "items[\"item1\"] = \"item_name1\"\n",
    "items[\"item2\"] = \"item_name2\"\n",
    "items[\"item3\"] = \"item_name3\"\n",
    " \n",
    "group_data[\"items\"] = items\n",
    " \n",
    "# Print JSON\n",
    "print(json.dumps(group_data, ensure_ascii=False, indent=\"\\t\") )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
